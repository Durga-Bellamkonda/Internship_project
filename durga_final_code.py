# -*- coding: utf-8 -*-
"""Durga Final Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bOCfR4UAej3x4CyswIBizvb1VoPm6zwr
"""

#import pictures from drive 
from google.colab import drive
drive.mount('/content/drive')

!unzip -uq "/content/drive/My Drive/Data/skin-cancer-malignant-vs-benign.zip" -d "/content/drive/My Drive/Data"

#Upgrade memory - high RAM run time
#a = []
#while(1):
   # a.append(â€˜1â€™)

#Import some necessary Modules
import os
import cv2
from tensorflow import keras
import numpy as np
import pandas as pd
import random as rn
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt
from IPython.display import SVG
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

#loading pictures 
def Dataset_loader(DIR,RESIZE):
    IMG = []
    read = lambda imname: np.asarray(Image.open(imname).convert("RGB"))
    for IMAGE_NAME in tqdm(os.listdir(DIR)):
        PATH = os.path.join(DIR,IMAGE_NAME)
        _, ftype = os.path.splitext(PATH)
        if ftype == ".jpg":
            img = read(PATH)
            img = cv2.resize(img, (RESIZE,RESIZE))
            IMG.append(np.array(img)/255.)
    return IMG

benign_train = np.array(Dataset_loader('/content/drive/My Drive/Data/train/benign',224))
malign_train = np.array(Dataset_loader('/content/drive/My Drive/Data/train/malignant',224))
benign_test = np.array(Dataset_loader('/content/drive/My Drive/Data/test/benign',224))
malign_test = np.array(Dataset_loader('/content/drive/My Drive/Data/test/malignant',224))

# Create labels
benign_train_label = np.zeros(len(benign_train))
malign_train_label = np.ones(len(malign_train))
benign_test_label = np.zeros(len(benign_test))
malign_test_label = np.ones(len(malign_test))

# Merge data 
X_train = np.concatenate((benign_train, malign_train), axis = 0)
Y_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)
X_test = np.concatenate((benign_test, malign_test), axis = 0)
Y_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)

# Shuffle train data
s = np.arange(X_train.shape[0])
np.random.shuffle(s)
X_train = X_train[s]
Y_train = Y_train[s]

# Shuffle test data
s = np.arange(X_test.shape[0])
np.random.shuffle(s)
X_test = X_test[s]
Y_test = Y_test[s]

# Split validation data from train data
x_train=X_train[500:]
x_val=X_train[:500]
y_train=Y_train[500:]
y_val=Y_train[:500]

X_test.shape, x_val.shape, x_train.shape

# Display first 20 images of moles

fig=plt.figure(figsize=(10, 10))

for i in range(1, 20 +1):
    ax = fig.add_subplot(4, 5, i)
    if Y_train[i] == 0:
        ax.title.set_text('Benign')
    else:
        ax.title.set_text('Malignant')
    plt.imshow(X_train[i], interpolation='nearest')
plt.show()

# one hot encoding 
from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encodin
y_train = to_categorical(y_train, num_classes= 2)
Y_test = to_categorical(Y_test, num_classes= 2)
y_val = to_categorical(y_val, num_classes= 2)

# normalization 
x_train = x_train/255.
X_test = X_test/255.
x_val = x_val/255.

X_test.shape, x_val.shape, x_train.shape

y_train.shape,Y_test.shape

# data augmentation - cut out 

import numpy as np

def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):
    def eraser(input_img):
        img_h, img_w, img_c = input_img.shape
        p_1 = np.random.rand()

        if p_1 > p:
            return input_img

        while True:
            s = np.random.uniform(s_l, s_h) * img_h * img_w
            r = np.random.uniform(r_1, r_2)
            w = int(np.sqrt(s / r))
            h = int(np.sqrt(s * r))
            left = np.random.randint(0, img_w)
            top = np.random.randint(0, img_h)

            if left + w <= img_w and top + h <= img_h:
                break

        if pixel_level:
            c = np.random.uniform(v_l, v_h, (h, w, img_c))
        else:
            c = np.random.uniform(v_l, v_h)

        input_img[top:top + h, left:left + w, :] = c

        return input_img

    return eraser

# data augmentation 

from tensorflow.keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=45,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image 
        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True,  # randomly flip images
        preprocessing_function=get_random_eraser(p=0.5, v_h=0, s_h=0.25))

#%tensorflow_version 1.x
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras import datasets
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout
#from tensorflow.keras.layers.normalization import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.regularizers import l2
from tensorflow.keras import optimizers

#!pip install tensorflow==2.1.0
#!pip install keras==2.3.1
#!pip install segmentation-models



#pip install segmentation_models

#from tensorflow.keras import segmentation_models as sm

#sm.set_framework('tf.keras')

#sm.framework()











# building the model 

#!pip install -U efficientnet
from tensorflow.keras import layers
#import efficientnet.keras as efn
from tensorflow.keras import regularizers 
from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten

tf.keras.applications.efficientnet.EfficientNetB0(
    include_top=True, weights='imagenet', input_tensor=None,
    input_shape=(224,224,3), pooling=None, classes=1000,
    classifier_activation='softmax'
)
model=Sequential()
model.add(Conv2D(15, (5,5), input_shape = (224, 224,3), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(30, (5,5), activation = 'relu'))
model.add(layers.MaxPooling2D(pool_size=(2,2)))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dropout(0.2))
#model.add(Dense(256, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

"""
alpha = 1e-3  # weight decay coefficient
for layer in model.layers:
    if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):
        layer.add_loss(keras.regularizers.l2(alpha)(layer.kernel))
    if hasattr(layer, 'bias_regularizer') and layer.use_bias:
        layer.add_loss(keras.regularizers.l2(alpha)(layer.bias))
"""
model.summary()



# vizualizing the model 
from tensorflow.keras.utils import plot_model
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

# callback - if accuracy doesn't improve after 3 epoch then reduce lr by factor 0.5 
reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,patience=3, min_lr=0.000005)

tf.keras.applications.efficientnet.EfficientNetB0(
    include_top=True, weights='imagenet', input_tensor=None,
    input_shape=(224,224,3), pooling=None, classes=1000,
    classifier_activation='softmax'
)

# train the model 
optimizer=optimizers.Adam()
model.compile(optimizer=optimizer,loss="binary_crossentropy", metrics=['accuracy'])
#h = model.fit(x_train,y_train, batch_size=64,epochs=100,callbacks=[reduce_lr])
h = model.fit(x_train,y_train, batch_size=64,epochs=100)

# loss and validation loss
import matplotlib.pyplot as plt
plt.figure(figsize=(20,8))
plt.plot(h.history['loss'],color="#F7728B", label='loss')
#plt.plot(h.history['val_loss'], color="#3CA3EC", label='validation loss')
plt.legend(loc="upper right",fancybox=True, framealpha=1, shadow=True, borderpad=1)
plt.title(label="Loss & Validation Loss")

# accuracy and validation accuracy
plt.figure(figsize=(20,8))
plt.plot(h.history['accuracy'],color="#51B232", label='accuracy')
#plt.plot(h.history['val_accuracy'], color="#CF8F32", label='validation accuracy')
plt.legend(loc="upper right",fancybox=True, framealpha=1, shadow=True, borderpad=1)
plt.title(label="Accuracy & Validation Accuracy")

# test the model 
results = model.evaluate(X_test, Y_test, batch_size=64)
print('test loss',results[0], 'test acc:',results[1])
results

# test the model
y_pred = model.predict(X_test, batch_size=64)
print(accuracy_score(np.argmax(Y_test, axis=1), np.argmax(y_pred, axis=1)))

y_pred[0], y_pred[0][0],y_pred[0][1]

#Confusion Matrix

pred_results = np.where(y_pred>=0.5, 1,y_pred) 
pred_results2 = np.where(pred_results<0.5, 0,pred_results)

import sklearn
from sklearn.metrics import confusion_matrix
cm= sklearn.metrics.confusion_matrix(Y_test.argmax(axis=1), pred_results2.argmax(axis=1))
print(cm)

import seaborn as sn
import matplotlib.pyplot as plt
sn.heatmap(cm, annot=True)
plt.show()

labels = ['benign', 'malignant']

fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm)
plt.title('Confusion matrix of the classifier')
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()